{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bdc7fb6-285e-476a-9a64-4fc0669374cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75338d1f-961a-43a7-88db-86652abaa62c",
   "metadata": {},
   "source": [
    "`nn.Flatten()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a83fea-ebc9-414c-8e14-b5bd06c13332",
   "metadata": {},
   "source": [
    "`nn.apply()`  \n",
    "\n",
    "### 代码解释\n",
    "\n",
    "```python\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, std=0.01)\n",
    "\n",
    "net.apply(init_weights)\n",
    "```\n",
    "\n",
    "1. **定义 `init_weights` 函数**:\n",
    "   ```python\n",
    "   def init_weights(m):\n",
    "       if type(m) == nn.Linear:\n",
    "           nn.init.normal_(m.weight, std=0.01)\n",
    "   ```\n",
    "   - **`def init_weights(m):`**: 定义了一个名为 `init_weights` 的函数，它接受一个参数 `m`，表示网络中的一个层（module）。\n",
    "   - **`if type(m) == nn.Linear:`**: 判断传入的层 `m` 是否为 `nn.Linear` 类型。`nn.Linear` 是一个全连接层（线性层）。\n",
    "   - **`nn.init.normal_(m.weight, std=0.01)`**: 使用正态分布（均值为0，标准差为0.01）来初始化 `m` 的权重。`nn.init.normal_` 是一个 PyTorch 的函数，用于在给定标准差的情况下对权重进行正态分布初始化。\n",
    "\n",
    "2. **应用初始化函数**:\n",
    "   ```python\n",
    "   net.apply(init_weights)\n",
    "   ```\n",
    "   - **`net`**: 这是你的神经网络模型（通常是 `nn.Module` 的子类）。\n",
    "   - **`net.apply(init_weights)`**: `apply` 是一个 PyTorch 的方法，用于将指定的函数 `init_weights` 应用到模型中的所有子模块（层）。该函数会递归地访问模型中的每个子层，检查其类型，并对符合条件的层应用初始化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254a3de1-94b1-4220-98f6-f6bb94161eb4",
   "metadata": {},
   "source": [
    "`nn.init.normal_` 和 `nn.init.normal`   \n",
    "\n",
    "是 PyTorch 中用于初始化张量的两个不同函数，它们的使用方式和功能有所不同。下面详细解释这两个函数的区别：\n",
    "\n",
    "### 1. `nn.init.normal_`\n",
    "\n",
    "**`nn.init.normal_`** 是一个**原地**（in-place）操作的初始化函数，用于对给定的张量进行正态分布初始化。\n",
    "\n",
    "**函数签名**:\n",
    "```python\n",
    "torch.nn.init.normal_(tensor, mean=0.0, std=1.0, generator=None)\n",
    "```\n",
    "\n",
    "- **`tensor`**: 需要初始化的张量。\n",
    "- **`mean`**: 正态分布的均值（默认为 0.0）。\n",
    "- **`std`**: 正态分布的标准差（默认为 1.0）。\n",
    "- **`generator`**: 可选的随机数生成器。\n",
    "\n",
    "**特点**:\n",
    "- **原地操作**: `normal_` 直接修改输入张量的值，因此它在张量上执行操作并改变其内容。\n",
    "- **使用**: 用于在初始化模型时对参数进行赋值。\n",
    "\n",
    "**示例**:\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "tensor = torch.empty(3, 3)  # 创建一个空的张量\n",
    "nn.init.normal_(tensor, mean=0.0, std=1.0)  # 对张量进行正态分布初始化\n",
    "print(tensor)\n",
    "```\n",
    "\n",
    "### 2. `nn.init.normal`\n",
    "\n",
    "**`nn.init.normal`** 是一个**创建新张量**的函数，用于生成具有正态分布的张量。\n",
    "\n",
    "**函数签名**:\n",
    "```python\n",
    "torch.nn.init.normal(tensor, mean=0.0, std=1.0, generator=None)\n",
    "```\n",
    "\n",
    "- **`tensor`**: 需要初始化的张量。\n",
    "- **`mean`**: 正态分布的均值（默认为 0.0）。\n",
    "- **`std`**: 正态分布的标准差（默认为 1.0）。\n",
    "- **`generator`**: 可选的随机数生成器。\n",
    "\n",
    "**特点**:\n",
    "- **创建新张量**: `normal` 函数用于创建一个新的张量，初始化为具有给定正态分布的值。\n",
    "- **返回值**: `normal` 函数返回一个新张量，而不会改变原始张量。\n",
    "\n",
    "**示例**:\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 创建一个新张量并进行正态分布初始化\n",
    "tensor = torch.empty(3, 3)\n",
    "tensor = nn.init.normal(tensor, mean=0.0, std=1.0)\n",
    "print(tensor)\n",
    "```\n",
    "\n",
    "### 总结\n",
    "\n",
    "- **`nn.init.normal_`**:\n",
    "  - 是原地操作函数，直接修改给定的张量。\n",
    "  - 适用于在初始化过程中直接对张量进行赋值。\n",
    "  \n",
    "- **`nn.init.normal`**:\n",
    "  - 创建一个新的张量，并对其进行正态分布初始化。\n",
    "  - 返回初始化后的新张量，而不会修改原始张量。\n",
    "\n",
    "在实际使用中，`nn.init.normal_` 是用于对模型的参数进行初始化的标准方法，因为模型的参数通常需要直接在原地初始化。而 `nn.init.normal` 更多用于生成新的张量进行初始化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83778713-6216-4855-8512-7d5daa2d63de",
   "metadata": {},
   "source": [
    "`nn.CrossEntropyLoss()` 是 PyTorch 中的一个损失函数，用于计算分类任务中的交叉熵损失。下面是对该函数输入、参数和返回值的详细解释：\n",
    "\n",
    "### 函数定义\n",
    "\n",
    "```python\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "```\n",
    "\n",
    "### 输入\n",
    "\n",
    "1. **`input`**:\n",
    "   - **类型**: Tensor\n",
    "   - **形状**: `(N, C)`，其中 `N` 是批次大小（batch size），`C` 是类别数量（number of classes）。\n",
    "   - **描述**: 模型的原始输出（logits），即未经过 softmax 的预测分数。每个元素表示一个类别的预测分数。\n",
    "\n",
    "2. **`target`**:\n",
    "   - **类型**: Tensor\n",
    "   - **形状**: `(N,)`\n",
    "   - **描述**: 实际的目标标签（ground truth labels），每个值是目标类别的索引。目标类别的值是从 `0` 到 `C-1` 的整数。\n",
    "\n",
    "### 参数\n",
    "\n",
    "`nn.CrossEntropyLoss()` 可以接收以下可选参数：\n",
    "\n",
    "- **`weight`**: Tensor, 可选。用于加权损失的权重。它的形状应该是 `(C,)`，与类别数相匹配。可以用来处理类别不平衡的问题，给不同的类别分配不同的权重。\n",
    "\n",
    "- **`size_average`**: Bool, 可选。默认值是 `True`。如果为 `True`，则返回损失的均值；如果为 `False`，则返回损失的总和。这个参数在 PyTorch 的较早版本中使用，最新版本中已被 `reduction` 替代。\n",
    "\n",
    "- **`reduce`**: Bool, 可选。默认值是 `True`。如果为 `True`，则返回损失的均值或总和；如果为 `False`，则返回每个样本的损失。这个参数在 PyTorch 的较早版本中使用，最新版本中已被 `reduction` 替代。\n",
    "\n",
    "- **`reduction`**: 字符串，可选。默认值是 `'mean'`。用于指定损失的归约方式：\n",
    "  - `'none'`：不进行归约，返回每个样本的损失。\n",
    "  - `'mean'`：返回所有样本损失的均值。\n",
    "  - `'sum'`：返回所有样本损失的总和。\n",
    "\n",
    "### 返回值\n",
    "\n",
    "- **类型**: Tensor\n",
    "- **形状**: 如果 `reduction='none'`，返回 `(N,)` 形状的 Tensor；如果 `reduction='mean'` 或 `reduction='sum'`，返回一个标量 Tensor。\n",
    "- **描述**: 计算得到的损失值。根据 `reduction` 参数的设置，返回每个样本的损失、所有样本损失的均值或总和。\n",
    "\n",
    "### 示例代码\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 创建交叉熵损失函数对象\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# 模拟模型的预测输出（logits）和目标标签\n",
    "outputs = torch.tensor([[1.0, 2.0, 0.5], [0.5, 1.0, 2.0]])\n",
    "targets = torch.tensor([1, 2])\n",
    "\n",
    "# 计算损失\n",
    "loss = loss_fn(outputs, targets)\n",
    "print(loss)  # 输出: 计算得到的损失值\n",
    "```\n",
    "\n",
    "### 计算步骤\n",
    "\n",
    "1. **Softmax**: `nn.CrossEntropyLoss()` 内部自动应用 softmax 操作，将 logits 转换为概率分布。\n",
    "2. **负对数似然**: 计算目标类别的对数概率，并取其负值。\n",
    "3. **损失归约**: 根据 `reduction` 参数的设置，对损失值进行归约处理，返回最终的损失值。\n",
    "\n",
    "### 总结\n",
    "\n",
    "`nn.CrossEntropyLoss()` 是用于计算分类任务中的交叉熵损失的函数。它结合了 softmax 和负对数似然损失，并提供了多种损失归约方式，适用于多类分类问题。通过设置可选参数，可以对损失进行加权和归约，处理复杂的损失计算需求。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095ddc93-e5bd-44a5-b459-58945f8f6074",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
