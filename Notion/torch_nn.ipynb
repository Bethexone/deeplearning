{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bdc7fb6-285e-476a-9a64-4fc0669374cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75338d1f-961a-43a7-88db-86652abaa62c",
   "metadata": {},
   "source": [
    "`nn.Flatten()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a83fea-ebc9-414c-8e14-b5bd06c13332",
   "metadata": {},
   "source": [
    "`nn.apply()`  \n",
    "\n",
    "### 代码解释\n",
    "\n",
    "```python\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, std=0.01)\n",
    "\n",
    "net.apply(init_weights)\n",
    "```\n",
    "\n",
    "1. **定义 `init_weights` 函数**:\n",
    "   ```python\n",
    "   def init_weights(m):\n",
    "       if type(m) == nn.Linear:\n",
    "           nn.init.normal_(m.weight, std=0.01)\n",
    "   ```\n",
    "   - **`def init_weights(m):`**: 定义了一个名为 `init_weights` 的函数，它接受一个参数 `m`，表示网络中的一个层（module）。\n",
    "   - **`if type(m) == nn.Linear:`**: 判断传入的层 `m` 是否为 `nn.Linear` 类型。`nn.Linear` 是一个全连接层（线性层）。\n",
    "   - **`nn.init.normal_(m.weight, std=0.01)`**: 使用正态分布（均值为0，标准差为0.01）来初始化 `m` 的权重。`nn.init.normal_` 是一个 PyTorch 的函数，用于在给定标准差的情况下对权重进行正态分布初始化。\n",
    "\n",
    "2. **应用初始化函数**:\n",
    "   ```python\n",
    "   net.apply(init_weights)\n",
    "   ```\n",
    "   - **`net`**: 这是你的神经网络模型（通常是 `nn.Module` 的子类）。\n",
    "   - **`net.apply(init_weights)`**: `apply` 是一个 PyTorch 的方法，用于将指定的函数 `init_weights` 应用到模型中的所有子模块（层）。该函数会递归地访问模型中的每个子层，检查其类型，并对符合条件的层应用初始化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254a3de1-94b1-4220-98f6-f6bb94161eb4",
   "metadata": {},
   "source": [
    "`nn.init.normal_` 和 `nn.init.normal`   \n",
    "\n",
    "是 PyTorch 中用于初始化张量的两个不同函数，它们的使用方式和功能有所不同。下面详细解释这两个函数的区别：\n",
    "\n",
    "### 1. `nn.init.normal_`\n",
    "\n",
    "**`nn.init.normal_`** 是一个**原地**（in-place）操作的初始化函数，用于对给定的张量进行正态分布初始化。\n",
    "\n",
    "**函数签名**:\n",
    "```python\n",
    "torch.nn.init.normal_(tensor, mean=0.0, std=1.0, generator=None)\n",
    "```\n",
    "\n",
    "- **`tensor`**: 需要初始化的张量。\n",
    "- **`mean`**: 正态分布的均值（默认为 0.0）。\n",
    "- **`std`**: 正态分布的标准差（默认为 1.0）。\n",
    "- **`generator`**: 可选的随机数生成器。\n",
    "\n",
    "**特点**:\n",
    "- **原地操作**: `normal_` 直接修改输入张量的值，因此它在张量上执行操作并改变其内容。\n",
    "- **使用**: 用于在初始化模型时对参数进行赋值。\n",
    "\n",
    "**示例**:\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "tensor = torch.empty(3, 3)  # 创建一个空的张量\n",
    "nn.init.normal_(tensor, mean=0.0, std=1.0)  # 对张量进行正态分布初始化\n",
    "print(tensor)\n",
    "```\n",
    "\n",
    "### 2. `nn.init.normal`\n",
    "\n",
    "**`nn.init.normal`** 是一个**创建新张量**的函数，用于生成具有正态分布的张量。\n",
    "\n",
    "**函数签名**:\n",
    "```python\n",
    "torch.nn.init.normal(tensor, mean=0.0, std=1.0, generator=None)\n",
    "```\n",
    "\n",
    "- **`tensor`**: 需要初始化的张量。\n",
    "- **`mean`**: 正态分布的均值（默认为 0.0）。\n",
    "- **`std`**: 正态分布的标准差（默认为 1.0）。\n",
    "- **`generator`**: 可选的随机数生成器。\n",
    "\n",
    "**特点**:\n",
    "- **创建新张量**: `normal` 函数用于创建一个新的张量，初始化为具有给定正态分布的值。\n",
    "- **返回值**: `normal` 函数返回一个新张量，而不会改变原始张量。\n",
    "\n",
    "**示例**:\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 创建一个新张量并进行正态分布初始化\n",
    "tensor = torch.empty(3, 3)\n",
    "tensor = nn.init.normal(tensor, mean=0.0, std=1.0)\n",
    "print(tensor)\n",
    "```\n",
    "\n",
    "### 总结\n",
    "\n",
    "- **`nn.init.normal_`**:\n",
    "  - 是原地操作函数，直接修改给定的张量。\n",
    "  - 适用于在初始化过程中直接对张量进行赋值。\n",
    "  \n",
    "- **`nn.init.normal`**:\n",
    "  - 创建一个新的张量，并对其进行正态分布初始化。\n",
    "  - 返回初始化后的新张量，而不会修改原始张量。\n",
    "\n",
    "在实际使用中，`nn.init.normal_` 是用于对模型的参数进行初始化的标准方法，因为模型的参数通常需要直接在原地初始化。而 `nn.init.normal` 更多用于生成新的张量进行初始化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83778713-6216-4855-8512-7d5daa2d63de",
   "metadata": {},
   "source": [
    "`nn.CrossEntropyLoss()` 是 PyTorch 中的一个损失函数，用于计算分类任务中的交叉熵损失。下面是对该函数输入、参数和返回值的详细解释：\n",
    "\n",
    "### 函数定义\n",
    "\n",
    "```python\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "```\n",
    "\n",
    "### 输入\n",
    "\n",
    "1. **`input`**:\n",
    "   - **类型**: Tensor\n",
    "   - **形状**: `(N, C)`，其中 `N` 是批次大小（batch size），`C` 是类别数量（number of classes）。\n",
    "   - **描述**: 模型的原始输出（logits），即未经过 softmax 的预测分数。每个元素表示一个类别的预测分数。\n",
    "\n",
    "2. **`target`**:\n",
    "   - **类型**: Tensor\n",
    "   - **形状**: `(N,)`\n",
    "   - **描述**: 实际的目标标签（ground truth labels），每个值是目标类别的索引。目标类别的值是从 `0` 到 `C-1` 的整数。\n",
    "\n",
    "### 参数\n",
    "\n",
    "`nn.CrossEntropyLoss()` 可以接收以下可选参数：\n",
    "\n",
    "- **`weight`**: Tensor, 可选。用于加权损失的权重。它的形状应该是 `(C,)`，与类别数相匹配。可以用来处理类别不平衡的问题，给不同的类别分配不同的权重。\n",
    "\n",
    "- **`size_average`**: Bool, 可选。默认值是 `True`。如果为 `True`，则返回损失的均值；如果为 `False`，则返回损失的总和。这个参数在 PyTorch 的较早版本中使用，最新版本中已被 `reduction` 替代。\n",
    "\n",
    "- **`reduce`**: Bool, 可选。默认值是 `True`。如果为 `True`，则返回损失的均值或总和；如果为 `False`，则返回每个样本的损失。这个参数在 PyTorch 的较早版本中使用，最新版本中已被 `reduction` 替代。\n",
    "\n",
    "- **`reduction`**: 字符串，可选。默认值是 `'mean'`。用于指定损失的归约方式：\n",
    "  - `'none'`：不进行归约，返回每个样本的损失。\n",
    "  - `'mean'`：返回所有样本损失的均值。\n",
    "  - `'sum'`：返回所有样本损失的总和。\n",
    "\n",
    "### 返回值\n",
    "\n",
    "- **类型**: Tensor\n",
    "- **形状**: 如果 `reduction='none'`，返回 `(N,)` 形状的 Tensor；如果 `reduction='mean'` 或 `reduction='sum'`，返回一个标量 Tensor。\n",
    "- **描述**: 计算得到的损失值。根据 `reduction` 参数的设置，返回每个样本的损失、所有样本损失的均值或总和。\n",
    "\n",
    "### 示例代码\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 创建交叉熵损失函数对象\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# 模拟模型的预测输出（logits）和目标标签\n",
    "outputs = torch.tensor([[1.0, 2.0, 0.5], [0.5, 1.0, 2.0]])\n",
    "targets = torch.tensor([1, 2])\n",
    "\n",
    "# 计算损失\n",
    "loss = loss_fn(outputs, targets)\n",
    "print(loss)  # 输出: 计算得到的损失值\n",
    "```\n",
    "\n",
    "### 计算步骤\n",
    "\n",
    "1. **Softmax**: `nn.CrossEntropyLoss()` 内部自动应用 softmax 操作，将 logits 转换为概率分布。\n",
    "2. **负对数似然**: 计算目标类别的对数概率，并取其负值。\n",
    "3. **损失归约**: 根据 `reduction` 参数的设置，对损失值进行归约处理，返回最终的损失值。\n",
    "\n",
    "### 总结\n",
    "\n",
    "`nn.CrossEntropyLoss()` 是用于计算分类任务中的交叉熵损失的函数。它结合了 softmax 和负对数似然损失，并提供了多种损失归约方式，适用于多类分类问题。通过设置可选参数，可以对损失进行加权和归约，处理复杂的损失计算需求。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f75ef69-688d-4c39-8f3d-ad48d9fec283",
   "metadata": {},
   "source": [
    "`nn.Parameter` 和 `m.weight` 之间的区别可以从两个层面来理解：概念层面和具体实现层面。\n",
    "\n",
    "### 1. `nn.Parameter` 的概念\n",
    "\n",
    "- **`nn.Parameter`** 是 PyTorch 中用于定义模型参数的特殊张量。它是 `torch.Tensor` 的一个子类，其主要作用是告诉 PyTorch 这个张量是一个需要被优化的参数。任何使用 `nn.Parameter` 定义的张量都会自动注册为 `nn.Module` 的一部分，并在调用 `model.parameters()` 时包含在返回的参数列表中。\n",
    "\n",
    "- **自动参与反向传播**：`nn.Parameter` 会自动参与反向传播，并且在优化过程中被更新。\n",
    "\n",
    "### 2. `m.weight` 的概念\n",
    "\n",
    "- **`m.weight`** 是特定层（例如 `nn.Linear`, `nn.Conv2d` 等）中的权重参数。在这些层中，权重通常是用 `nn.Parameter` 定义的，因此 `m.weight` 本质上是一个 `nn.Parameter` 对象。\n",
    "\n",
    "- **关联到具体的模型层**：`m.weight` 通常与某个特定的神经网络层相关联，是该层的权重矩阵。它是在构造特定层（如 `nn.Linear`）时被自动创建并初始化的。\n",
    "\n",
    "### 3. 区别与联系\n",
    "\n",
    "- **联系**：`m.weight` 通常是一个 `nn.Parameter`，它是某个具体层（例如 `nn.Linear`）中的权重矩阵。也就是说，`m.weight` 是由 `nn.Parameter` 定义的一个特殊张量。换句话说，`m.weight` 是 `nn.Parameter` 的一个实例。\n",
    "\n",
    "- **区别**：`nn.Parameter` 是一个用于定义任何模型参数的概念工具，而 `m.weight` 是特定层（如 `nn.Linear`）中的权重矩阵。`m.weight` 是 `nn.Parameter` 的具体应用场景。\n",
    "\n",
    "### 4. 示例代码\n",
    "\n",
    "以下代码展示了 `nn.Parameter` 和 `m.weight` 之间的关系：\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyLinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(MyLinearLayer, self).__init__()\n",
    "        # 使用 nn.Parameter 手动定义一个权重参数\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n",
    "        # 偏置参数也可以用 nn.Parameter 手动定义\n",
    "        self.bias = nn.Parameter(torch.randn(out_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x @ self.weight.t() + self.bias\n",
    "\n",
    "# 创建一个自定义的线性层\n",
    "layer = MyLinearLayer(10, 5)\n",
    "\n",
    "# 打印自定义层的权重和偏置\n",
    "print(layer.weight)  # 这是 nn.Parameter 类型的张量\n",
    "print(layer.bias)    # 也是 nn.Parameter 类型的张量\n",
    "\n",
    "# 调用 PyTorch 自带的 nn.Linear 层\n",
    "linear = nn.Linear(10, 5)\n",
    "\n",
    "# 打印 nn.Linear 层的权重和偏置\n",
    "print(linear.weight)  # 这是 nn.Parameter 类型的张量\n",
    "print(linear.bias)    # 也是 nn.Parameter 类型的张量\n",
    "```\n",
    "\n",
    "在这个示例中：\n",
    "\n",
    "- `layer.weight` 是手动使用 `nn.Parameter` 定义的权重张量。\n",
    "- `linear.weight` 是使用 `nn.Linear` 层自动生成的权重张量。虽然方式不同，但它们本质上都是 `nn.Parameter` 对象，并且都参与模型的训练。\n",
    "\n",
    "### 总结\n",
    "\n",
    "- **`nn.Parameter`** 是用于定义可训练模型参数的工具。\n",
    "- **`m.weight`** 是具体层中的权重参数，通常是通过 `nn.Parameter` 创建的张量。\n",
    "- 它们之间的关系是：`m.weight` 是一个由 `nn.Parameter` 定义的特定张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5fc4bfc-3fa0-4f20-a773-8fb6d34a3d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[ 0.1089, -0.2914, -0.4313,  0.1352, -0.1609],\n",
       "         [-0.1652, -0.1341, -0.1260, -0.3020, -0.2400],\n",
       "         [-0.3801,  0.0726, -0.0502,  0.3054,  0.4354],\n",
       "         [ 0.0056,  0.4245,  0.0022, -0.0084, -0.0978]], requires_grad=True),\n",
       " <generator object Module.parameters at 0x000001493B55C740>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "net=nn.Linear(5,4)\n",
    "net.weight,net.parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8284c0a-5e69-4caf-a76c-71affb32e943",
   "metadata": {},
   "source": [
    "`net.weight` 和 `net.parameters()` 都涉及到神经网络模型中的参数管理，但它们的用途和作用范围有所不同。下面解释它们之间的区别：\n",
    "\n",
    "### 1. `net.weight`\n",
    "- **具体的层参数**: `net.weight` 通常指某个特定层（如 `nn.Linear` 或 `nn.Conv2d`）的权重参数。例如，如果 `net` 是一个 `nn.Linear` 层，那么 `net.weight` 就是这个线性层的权重矩阵。\n",
    "\n",
    "- **单个张量**: `net.weight` 是一个 `torch.Tensor` 对象，通常是 `nn.Parameter` 类型，它表示具体层的权重数据。\n",
    "\n",
    "- **示例**:\n",
    "  ```python\n",
    "  import torch.nn as nn\n",
    "\n",
    "  net = nn.Linear(10, 5)  # 创建一个线性层\n",
    "  print(net.weight)  # 打印该层的权重参数\n",
    "  ```\n",
    "\n",
    "  这里的 `net.weight` 直接访问并返回线性层的权重参数。\n",
    "\n",
    "### 2. `net.parameters()`\n",
    "- **整个模型的所有参数**: `net.parameters()` 是一个方法，返回的是模型中所有可训练参数的迭代器。这些参数通常是通过 `nn.Parameter` 定义的，包括权重和偏置（如 `weight` 和 `bias`）。\n",
    "\n",
    "- **用于优化器**: 当你要将模型的所有参数传递给优化器进行训练时，通常会使用 `net.parameters()`。\n",
    "\n",
    "- **示例**:\n",
    "  ```python\n",
    "  import torch.nn as nn\n",
    "\n",
    "  class SimpleModel(nn.Module):\n",
    "      def __init__(self):\n",
    "          super(SimpleModel, self).__init__()\n",
    "          self.fc1 = nn.Linear(10, 5)\n",
    "          self.fc2 = nn.Linear(5, 2)\n",
    "\n",
    "      def forward(self, x):\n",
    "          x = self.fc1(x)\n",
    "          return self.fc2(x)\n",
    "\n",
    "  net = SimpleModel()\n",
    "  for param in net.parameters():\n",
    "      print(param)\n",
    "  ```\n",
    "\n",
    "  在这个示例中，`net.parameters()` 返回的是模型 `net` 中所有层的参数，包括 `fc1` 和 `fc2` 层的权重和偏置。\n",
    "\n",
    "### 3. 区别总结\n",
    "- **范围**:\n",
    "  - `net.weight` 是针对某个特定层的权重参数。\n",
    "  - `net.parameters()` 返回整个模型的所有可训练参数，包括多个层的权重和偏置。\n",
    "\n",
    "- **返回类型**:\n",
    "  - `net.weight` 是一个单独的 `torch.Tensor` 对象。\n",
    "  - `net.parameters()` 是一个生成器对象，生成模型中所有参数的迭代器。\n",
    "\n",
    "- **典型用法**:\n",
    "  - `net.weight` 通常在特定的层上被使用，用于直接访问该层的权重。\n",
    "  - `net.parameters()` 通常用于将整个模型的参数传递给优化器进行训练。\n",
    "\n",
    "### 示例比较\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "net = nn.Linear(10, 5)  # 创建一个线性层\n",
    "\n",
    "# 访问该层的权重\n",
    "print(\"Weight of the layer:\")\n",
    "print(net.weight)\n",
    "\n",
    "# 访问模型的所有参数（这里仅有一个线性层）\n",
    "print(\"\\nAll parameters of the model:\")\n",
    "for param in net.parameters():\n",
    "    print(param)\n",
    "```\n",
    "\n",
    "在上面的例子中：\n",
    "\n",
    "- `net.weight` 只打印该线性层的权重。\n",
    "- `net.parameters()` 则打印了模型中的所有参数，包括权重和偏置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4b357e8-7ea7-44de-9fc1-06b91d03f552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight of the layer:\n",
      "Parameter containing:\n",
      "tensor([[-0.3095, -0.2780, -0.0556, -0.2557, -0.0638,  0.1078,  0.3099,  0.1961,\n",
      "         -0.2684,  0.2785],\n",
      "        [ 0.2899, -0.2258,  0.2364, -0.0971,  0.0755,  0.1001, -0.2816, -0.2798,\n",
      "         -0.2820, -0.1501],\n",
      "        [ 0.2377, -0.2687,  0.0243,  0.1423, -0.1326, -0.0271, -0.0573,  0.1563,\n",
      "         -0.2458, -0.2427],\n",
      "        [ 0.2989, -0.0917, -0.0844, -0.2694,  0.1515,  0.2031,  0.0707,  0.2065,\n",
      "          0.1399,  0.1202],\n",
      "        [ 0.1425, -0.2385, -0.1261, -0.1842, -0.2102,  0.1136, -0.2575,  0.0577,\n",
      "          0.2284,  0.0769]], requires_grad=True)\n",
      "<generator object Module.parameters at 0x000002C5E299FCA0>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "net = nn.Linear(10, 5)  # 创建一个线性层\n",
    "\n",
    "# 访问该层的权重\n",
    "print(\"Weight of the layer:\")\n",
    "print(net.weight)\n",
    "print(net.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39701605-1acb-4f73-9c66-a2ad7c1ca228",
   "metadata": {},
   "source": [
    "这段代码展示了如何使用 `TensorDataset` 将数据集分为训练集和测试集。\n",
    "\n",
    "### 代码解释\n",
    "\n",
    "假设你有一个大的数据集，其中 `features` 是特征张量，`labels` 是标签张量。你希望将前 100 个样本作为训练数据，接下来的 100 个样本作为测试数据。通过使用 `TensorDataset`，你可以轻松创建对应的训练和测试数据集。\n",
    "\n",
    "### 详细说明\n",
    "\n",
    "```python\n",
    "train_dataset = TensorDataset(features[0:100], labels[0:100])\n",
    "test_dataset = TensorDataset(features[100:200], labels[100:200])\n",
    "```\n",
    "\n",
    "- **`features[0:100]`**: 取出 `features` 张量中的前 100 个样本，作为训练集的特征。\n",
    "- **`labels[0:100]`**: 取出对应的 `labels` 张量中的前 100 个标签，作为训练集的标签。\n",
    "- **`features[100:200]`**: 取出 `features` 张量中的第 101 到 200 个样本，作为测试集的特征。\n",
    "- **`labels[100:200]`**: 取出对应的 `labels` 张量中的第 101 到 200 个标签，作为测试集的标签。\n",
    "\n",
    "### 结果\n",
    "\n",
    "- **`train_dataset`**: 包含了前 100 个样本及其标签，可以用于模型的训练。\n",
    "- **`test_dataset`**: 包含了接下来的 100 个样本及其标签，可以用于模型的测试。\n",
    "\n",
    "### 后续操作\n",
    "\n",
    "创建了 `TensorDataset` 之后，你可以使用 `DataLoader` 来加载数据进行训练和测试。例如：\n",
    "\n",
    "```python\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# 迭代 train_loader，进行训练\n",
    "for features_batch, labels_batch in train_loader:\n",
    "    # 执行训练操作\n",
    "    pass\n",
    "\n",
    "# 迭代 test_loader，进行测试\n",
    "for features_batch, labels_batch in test_loader:\n",
    "    # 执行测试操作\n",
    "    pass\n",
    "```\n",
    "\n",
    "通过 `DataLoader`，你可以以小批量的形式加载数据，从而在模型训练或测试时节省内存并加快迭代速度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b79d96a-0c5b-44ae-bf56-97417dbfc7b0",
   "metadata": {},
   "source": [
    "`nn.MSELoss()` 是 PyTorch 中用于计算均方误差（Mean Squared Error, MSE）的损失函数。它常用于回归任务中，衡量预测值与真实值之间的平均平方差。\n",
    "\n",
    "### 使用示例\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 创建 MSELoss 对象\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# 假设有两个张量，表示预测值和真实值\n",
    "predictions = torch.tensor([0.5, 0.8, 1.0], dtype=torch.float32)\n",
    "targets = torch.tensor([0.0, 1.0, 1.0], dtype=torch.float32)\n",
    "\n",
    "# 计算损失\n",
    "loss = loss_fn(predictions, targets)\n",
    "\n",
    "print(\"MSE Loss:\", loss.item())\n",
    "```\n",
    "\n",
    "### 解释\n",
    "- `predictions` 是模型输出的预测值。\n",
    "- `targets` 是实际的标签或目标值。\n",
    "- `nn.MSELoss()` 计算预测值与目标值之间的平均平方差异，并返回一个标量值，表示损失。\n",
    "\n",
    "### 计算公式\n",
    "\n",
    "MSE 的公式为：\n",
    "\n",
    "\\[\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "\\]\n",
    "\n",
    "其中 \\( y_i \\) 是目标值， \\( \\hat{y}_i \\) 是预测值， \\( n \\) 是样本的数量。\n",
    "\n",
    "### 应用场景\n",
    "`nn.MSELoss()` 通常用于回归任务中，比如预测连续变量（如房价、温度等）时，计算模型的损失。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a32e537-f37c-40e2-a87f-9fdaf68312fe",
   "metadata": {},
   "source": [
    "`nn.Module` 是 PyTorch 中所有神经网络模型的基类。无论是简单的线性层、卷积层，还是复杂的自定义模型，都需要继承自 `nn.Module`。它提供了一些核心功能，如参数管理、前向传播定义、模型保存与加载等。\n",
    "\n",
    "### 核心概念\n",
    "\n",
    "1. **继承 `nn.Module`**：\n",
    "   - 每个自定义的神经网络模型都需要继承自 `nn.Module`。这使得你的模型具备了 PyTorch 的自动微分和其他功能。\n",
    "\n",
    "   ```python\n",
    "   import torch.nn as nn\n",
    "\n",
    "   class MyModel(nn.Module):\n",
    "       def __init__(self):\n",
    "           super(MyModel, self).__init__()\n",
    "           # 定义网络层\n",
    "           self.layer1 = nn.Linear(10, 5)\n",
    "           self.layer2 = nn.Linear(5, 1)\n",
    "       \n",
    "       def forward(self, x):\n",
    "           x = self.layer1(x)\n",
    "           x = self.layer2(x)\n",
    "           return x\n",
    "   ```\n",
    "\n",
    "2. **`__init__` 方法**：\n",
    "   - 在 `__init__` 方法中，定义模型的所有层和子模块。可以使用 PyTorch 提供的各种层，如 `nn.Linear`、`nn.Conv2d`、`nn.ReLU` 等。\n",
    "\n",
    "3. **`forward` 方法**：\n",
    "   - `forward` 方法定义了数据如何通过网络进行前向传播。在这个方法中，你应该使用在 `__init__` 中定义的层和函数，将输入数据逐层传递。\n",
    "\n",
    "4. **参数管理**：\n",
    "   - `nn.Module` 会自动将所有子模块的参数注册到模型中。可以通过 `model.parameters()` 或 `model.named_parameters()` 方法获取模型的所有参数或带名称的参数。\n",
    "\n",
    "   ```python\n",
    "   model = MyModel()\n",
    "   print(list(model.parameters()))  # 打印模型的所有参数\n",
    "   ```\n",
    "\n",
    "5. **注册子模块**：\n",
    "   - 当你在 `__init__` 方法中将一个层（如 `nn.Linear`）赋值给 `self` 的某个属性时，PyTorch 会自动将该层注册为子模块，并将其参数包含在 `model.parameters()` 中。\n",
    "\n",
    "6. **模型训练与评估**：\n",
    "   - PyTorch 提供了 `model.train()` 和 `model.eval()` 方法来切换模型的模式。这些方法主要用于在训练和推理时控制一些特殊层的行为，比如 `Dropout` 和 `BatchNorm`。\n",
    "   ```python\n",
    "   model.train()  # 切换到训练模式\n",
    "   model.eval()   # 切换到评估模式\n",
    "   ```\n",
    "\n",
    "7. **保存与加载模型**：\n",
    "   - PyTorch 使用 `torch.save()` 和 `torch.load()` 函数来保存和加载模型的状态字典。\n",
    "   ```python\n",
    "   torch.save(model.state_dict(), 'model.pth')  # 保存模型参数\n",
    "   model.load_state_dict(torch.load('model.pth'))  # 加载模型参数\n",
    "   ```\n",
    "\n",
    "### `nn.Module` 的优点\n",
    "- **层次化管理**：将复杂的神经网络模块化，每个模块都可以是一个 `nn.Module`，从而实现复杂模型的分层管理。\n",
    "- **自动微分**：所有通过 `nn.Module` 注册的参数都可以参与自动微分，从而简化了反向传播的实现。\n",
    "- **灵活性**：通过继承 `nn.Module`，你可以轻松地自定义新的层、损失函数、优化器等。\n",
    "\n",
    "通过继承 `nn.Module`，你可以构建从简单到复杂的神经网络模型，并享受 PyTorch 提供的强大功能，如自动微分、GPU 加速和易于调试的 API。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd38e142-25f1-457a-a5ec-fc092838551a",
   "metadata": {},
   "source": [
    "在 PyTorch 中，继承自 `torch.nn.Module` 的类可以直接使用类实例来调用 `forward` 方法，这是因为 PyTorch 通过重载（override）了 Python 的特殊方法 `__call__`，使得对象实例可以像函数一样被调用。\n",
    "\n",
    "### 详细解释\n",
    "\n",
    "1. **`__call__` 方法**：\n",
    "   - 在 Python 中，`__call__` 是一个特殊方法，它允许一个对象像函数一样被调用。当你调用 `model(x)` 时，实际上是调用了 `model.__call__(x)`。\n",
    "\n",
    "2. **`nn.Module.__call__` 的作用**：\n",
    "   - PyTorch 在 `nn.Module` 中重载了 `__call__` 方法，使得它在被调用时自动执行一系列操作，包括：\n",
    "     - **前向传播**：调用对象的 `forward(x)` 方法，执行前向传播计算。\n",
    "     - **钩子操作**：处理注册的前向钩子（forward hooks）和后向钩子（backward hooks）。\n",
    "     - **模式管理**：根据模型当前的状态（训练或评估模式），调整某些层的行为，如 `Dropout` 和 `BatchNorm`。\n",
    "\n",
    "   这些操作使得我们在编写代码时，只需调用 `model(x)`，PyTorch 就会自动执行 `forward(x)` 方法，而不需要手动调用。\n",
    "\n",
    "3. **示例**：\n",
    "\n",
    "   ```python\n",
    "   import torch\n",
    "   import torch.nn as nn\n",
    "\n",
    "   class MyModel(nn.Module):\n",
    "       def __init__(self):\n",
    "           super(MyModel, self).__init__()\n",
    "           self.layer = nn.Linear(10, 5)\n",
    "\n",
    "       def forward(self, x):\n",
    "           return self.layer(x)\n",
    "\n",
    "   model = MyModel()\n",
    "   x = torch.randn(1, 10)\n",
    "   output = model(x)  # 等价于 output = model.__call__(x)\n",
    "   print(output)\n",
    "   ```\n",
    "\n",
    "   在这个示例中，当你调用 `model(x)` 时，PyTorch 实际上执行了 `model.__call__(x)`，该方法又进一步调用了 `model.forward(x)`。\n",
    "\n",
    "### 关键点\n",
    "\n",
    "- **封装**：通过重载 `__call__`，PyTorch 将一些复杂的操作封装在一起，简化了用户的调用流程。\n",
    "- **灵活性**：这种设计允许你在 `forward` 方法中定义前向传播逻辑，而无需关心调用时的具体实现细节。\n",
    "- **一致性**：无论是自定义的模型还是内置的模型（如 `nn.Linear`），都可以使用相同的调用方式。\n",
    "\n",
    "因此，使用 `torch.nn.Module` 的类可以直接使用类名（如 `model(x)`）来调用 `forward`，是由于 PyTorch 对 `__call__` 方法进行了重载。这种设计既方便了使用，又保持了代码的简洁性和可读性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff45aff-5ef3-4de5-9cda-1cda587314c9",
   "metadata": {},
   "source": [
    "要在自定义的算法层中使用 `net.parameters()` 来获取该层的参数，确保该层继承自 `nn.Module`，并且将所有需要训练的参数定义为 `nn.Parameter`。这样，当自定义层被添加到网络中时，PyTorch 会自动将其参数纳入 `net.parameters()` 中进行管理。\n",
    "\n",
    "### 步骤与示例\n",
    "\n",
    "1. **继承 `nn.Module`**：\n",
    "   - 定义一个自定义层，并继承自 `nn.Module`。\n",
    "\n",
    "2. **定义参数**：\n",
    "   - 在 `__init__` 方法中使用 `nn.Parameter` 来定义需要训练的参数。`nn.Parameter` 是一个特殊的张量，PyTorch 会自动将它们注册为模块的参数。\n",
    "\n",
    "3. **添加到模型中**：\n",
    "   - 将自定义层作为模型的一部分，在模型的 `__init__` 方法中实例化并赋值给 `self` 的某个属性。\n",
    "\n",
    "### 示例代码\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义自定义的算法层\n",
    "class MyCustomLayer(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MyCustomLayer, self).__init__()\n",
    "        # 定义需要训练的参数\n",
    "        self.weight = nn.Parameter(torch.randn(input_size, output_size))\n",
    "        self.bias = nn.Parameter(torch.randn(output_size))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 简单的线性变换：y = xW + b\n",
    "        return torch.matmul(x, self.weight) + self.bias\n",
    "\n",
    "# 定义模型，包含自定义的层\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.custom_layer = MyCustomLayer(input_size, hidden_size)\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.custom_layer(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "# 使用模型\n",
    "model = MyModel(input_size=10, hidden_size=5, output_size=1)\n",
    "print(\"Model Parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape)\n",
    "\n",
    "# 访问模型的所有参数\n",
    "parameters = list(model.parameters())\n",
    "print(\"\\nTotal number of parameters:\", len(parameters))\n",
    "```\n",
    "\n",
    "### 关键点解释\n",
    "\n",
    "1. **`nn.Parameter`**：\n",
    "   - 在自定义层中使用 `nn.Parameter` 将张量标记为可训练的参数。PyTorch 会自动将这些参数注册到模块中。\n",
    "\n",
    "2. **`named_parameters()` 和 `parameters()`**：\n",
    "   - `model.named_parameters()` 会返回一个包含参数名称和参数本身的迭代器，方便检查模型的每个参数。\n",
    "   - `model.parameters()` 则返回模型中所有参数的一个迭代器。\n",
    "\n",
    "3. **自定义层的参数管理**：\n",
    "   - 当你在模型中定义了自定义层，并将其作为模型的属性时（如 `self.custom_layer`），PyTorch 会自动将该层的参数纳入 `net.parameters()` 中。这意味着当你调用 `net.parameters()` 时，自定义层的参数也会被包含在内。\n",
    "\n",
    "通过这种方式，你可以灵活地在模型中引入自定义算法，并确保它们的参数能够参与训练和优化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cabb4f-a25e-4c9b-90ca-17ba5e433c15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
