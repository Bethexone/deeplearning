{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bdc7fb6-285e-476a-9a64-4fc0669374cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75338d1f-961a-43a7-88db-86652abaa62c",
   "metadata": {},
   "source": [
    "`nn.Flatten()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a83fea-ebc9-414c-8e14-b5bd06c13332",
   "metadata": {},
   "source": [
    "`nn.apply()`  \n",
    "\n",
    "### 代码解释\n",
    "\n",
    "```python\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, std=0.01)\n",
    "\n",
    "net.apply(init_weights)\n",
    "```\n",
    "\n",
    "1. **定义 `init_weights` 函数**:\n",
    "   ```python\n",
    "   def init_weights(m):\n",
    "       if type(m) == nn.Linear:\n",
    "           nn.init.normal_(m.weight, std=0.01)\n",
    "   ```\n",
    "   - **`def init_weights(m):`**: 定义了一个名为 `init_weights` 的函数，它接受一个参数 `m`，表示网络中的一个层（module）。\n",
    "   - **`if type(m) == nn.Linear:`**: 判断传入的层 `m` 是否为 `nn.Linear` 类型。`nn.Linear` 是一个全连接层（线性层）。\n",
    "   - **`nn.init.normal_(m.weight, std=0.01)`**: 使用正态分布（均值为0，标准差为0.01）来初始化 `m` 的权重。`nn.init.normal_` 是一个 PyTorch 的函数，用于在给定标准差的情况下对权重进行正态分布初始化。\n",
    "\n",
    "2. **应用初始化函数**:\n",
    "   ```python\n",
    "   net.apply(init_weights)\n",
    "   ```\n",
    "   - **`net`**: 这是你的神经网络模型（通常是 `nn.Module` 的子类）。\n",
    "   - **`net.apply(init_weights)`**: `apply` 是一个 PyTorch 的方法，用于将指定的函数 `init_weights` 应用到模型中的所有子模块（层）。该函数会递归地访问模型中的每个子层，检查其类型，并对符合条件的层应用初始化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254a3de1-94b1-4220-98f6-f6bb94161eb4",
   "metadata": {},
   "source": [
    "`nn.init.normal_` 和 `nn.init.normal`   \n",
    "\n",
    "是 PyTorch 中用于初始化张量的两个不同函数，它们的使用方式和功能有所不同。下面详细解释这两个函数的区别：\n",
    "\n",
    "### 1. `nn.init.normal_`\n",
    "\n",
    "**`nn.init.normal_`** 是一个**原地**（in-place）操作的初始化函数，用于对给定的张量进行正态分布初始化。\n",
    "\n",
    "**函数签名**:\n",
    "```python\n",
    "torch.nn.init.normal_(tensor, mean=0.0, std=1.0, generator=None)\n",
    "```\n",
    "\n",
    "- **`tensor`**: 需要初始化的张量。\n",
    "- **`mean`**: 正态分布的均值（默认为 0.0）。\n",
    "- **`std`**: 正态分布的标准差（默认为 1.0）。\n",
    "- **`generator`**: 可选的随机数生成器。\n",
    "\n",
    "**特点**:\n",
    "- **原地操作**: `normal_` 直接修改输入张量的值，因此它在张量上执行操作并改变其内容。\n",
    "- **使用**: 用于在初始化模型时对参数进行赋值。\n",
    "\n",
    "**示例**:\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "tensor = torch.empty(3, 3)  # 创建一个空的张量\n",
    "nn.init.normal_(tensor, mean=0.0, std=1.0)  # 对张量进行正态分布初始化\n",
    "print(tensor)\n",
    "```\n",
    "\n",
    "### 2. `nn.init.normal`\n",
    "\n",
    "**`nn.init.normal`** 是一个**创建新张量**的函数，用于生成具有正态分布的张量。\n",
    "\n",
    "**函数签名**:\n",
    "```python\n",
    "torch.nn.init.normal(tensor, mean=0.0, std=1.0, generator=None)\n",
    "```\n",
    "\n",
    "- **`tensor`**: 需要初始化的张量。\n",
    "- **`mean`**: 正态分布的均值（默认为 0.0）。\n",
    "- **`std`**: 正态分布的标准差（默认为 1.0）。\n",
    "- **`generator`**: 可选的随机数生成器。\n",
    "\n",
    "**特点**:\n",
    "- **创建新张量**: `normal` 函数用于创建一个新的张量，初始化为具有给定正态分布的值。\n",
    "- **返回值**: `normal` 函数返回一个新张量，而不会改变原始张量。\n",
    "\n",
    "**示例**:\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 创建一个新张量并进行正态分布初始化\n",
    "tensor = torch.empty(3, 3)\n",
    "tensor = nn.init.normal(tensor, mean=0.0, std=1.0)\n",
    "print(tensor)\n",
    "```\n",
    "\n",
    "### 总结\n",
    "\n",
    "- **`nn.init.normal_`**:\n",
    "  - 是原地操作函数，直接修改给定的张量。\n",
    "  - 适用于在初始化过程中直接对张量进行赋值。\n",
    "  \n",
    "- **`nn.init.normal`**:\n",
    "  - 创建一个新的张量，并对其进行正态分布初始化。\n",
    "  - 返回初始化后的新张量，而不会修改原始张量。\n",
    "\n",
    "在实际使用中，`nn.init.normal_` 是用于对模型的参数进行初始化的标准方法，因为模型的参数通常需要直接在原地初始化。而 `nn.init.normal` 更多用于生成新的张量进行初始化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83778713-6216-4855-8512-7d5daa2d63de",
   "metadata": {},
   "source": [
    "`nn.CrossEntropyLoss()` 是 PyTorch 中的一个损失函数，用于计算分类任务中的交叉熵损失。下面是对该函数输入、参数和返回值的详细解释：\n",
    "\n",
    "### 函数定义\n",
    "\n",
    "```python\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "```\n",
    "\n",
    "### 输入\n",
    "\n",
    "1. **`input`**:\n",
    "   - **类型**: Tensor\n",
    "   - **形状**: `(N, C)`，其中 `N` 是批次大小（batch size），`C` 是类别数量（number of classes）。\n",
    "   - **描述**: 模型的原始输出（logits），即未经过 softmax 的预测分数。每个元素表示一个类别的预测分数。\n",
    "\n",
    "2. **`target`**:\n",
    "   - **类型**: Tensor\n",
    "   - **形状**: `(N,)`\n",
    "   - **描述**: 实际的目标标签（ground truth labels），每个值是目标类别的索引。目标类别的值是从 `0` 到 `C-1` 的整数。\n",
    "\n",
    "### 参数\n",
    "\n",
    "`nn.CrossEntropyLoss()` 可以接收以下可选参数：\n",
    "\n",
    "- **`weight`**: Tensor, 可选。用于加权损失的权重。它的形状应该是 `(C,)`，与类别数相匹配。可以用来处理类别不平衡的问题，给不同的类别分配不同的权重。\n",
    "\n",
    "- **`size_average`**: Bool, 可选。默认值是 `True`。如果为 `True`，则返回损失的均值；如果为 `False`，则返回损失的总和。这个参数在 PyTorch 的较早版本中使用，最新版本中已被 `reduction` 替代。\n",
    "\n",
    "- **`reduce`**: Bool, 可选。默认值是 `True`。如果为 `True`，则返回损失的均值或总和；如果为 `False`，则返回每个样本的损失。这个参数在 PyTorch 的较早版本中使用，最新版本中已被 `reduction` 替代。\n",
    "\n",
    "- **`reduction`**: 字符串，可选。默认值是 `'mean'`。用于指定损失的归约方式：\n",
    "  - `'none'`：不进行归约，返回每个样本的损失。\n",
    "  - `'mean'`：返回所有样本损失的均值。\n",
    "  - `'sum'`：返回所有样本损失的总和。\n",
    "\n",
    "### 返回值\n",
    "\n",
    "- **类型**: Tensor\n",
    "- **形状**: 如果 `reduction='none'`，返回 `(N,)` 形状的 Tensor；如果 `reduction='mean'` 或 `reduction='sum'`，返回一个标量 Tensor。\n",
    "- **描述**: 计算得到的损失值。根据 `reduction` 参数的设置，返回每个样本的损失、所有样本损失的均值或总和。\n",
    "\n",
    "### 示例代码\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 创建交叉熵损失函数对象\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# 模拟模型的预测输出（logits）和目标标签\n",
    "outputs = torch.tensor([[1.0, 2.0, 0.5], [0.5, 1.0, 2.0]])\n",
    "targets = torch.tensor([1, 2])\n",
    "\n",
    "# 计算损失\n",
    "loss = loss_fn(outputs, targets)\n",
    "print(loss)  # 输出: 计算得到的损失值\n",
    "```\n",
    "\n",
    "### 计算步骤\n",
    "\n",
    "1. **Softmax**: `nn.CrossEntropyLoss()` 内部自动应用 softmax 操作，将 logits 转换为概率分布。\n",
    "2. **负对数似然**: 计算目标类别的对数概率，并取其负值。\n",
    "3. **损失归约**: 根据 `reduction` 参数的设置，对损失值进行归约处理，返回最终的损失值。\n",
    "\n",
    "### 总结\n",
    "\n",
    "`nn.CrossEntropyLoss()` 是用于计算分类任务中的交叉熵损失的函数。它结合了 softmax 和负对数似然损失，并提供了多种损失归约方式，适用于多类分类问题。通过设置可选参数，可以对损失进行加权和归约，处理复杂的损失计算需求。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f75ef69-688d-4c39-8f3d-ad48d9fec283",
   "metadata": {},
   "source": [
    "`nn.Parameter` 和 `m.weight` 之间的区别可以从两个层面来理解：概念层面和具体实现层面。\n",
    "\n",
    "### 1. `nn.Parameter` 的概念\n",
    "\n",
    "- **`nn.Parameter`** 是 PyTorch 中用于定义模型参数的特殊张量。它是 `torch.Tensor` 的一个子类，其主要作用是告诉 PyTorch 这个张量是一个需要被优化的参数。任何使用 `nn.Parameter` 定义的张量都会自动注册为 `nn.Module` 的一部分，并在调用 `model.parameters()` 时包含在返回的参数列表中。\n",
    "\n",
    "- **自动参与反向传播**：`nn.Parameter` 会自动参与反向传播，并且在优化过程中被更新。\n",
    "\n",
    "### 2. `m.weight` 的概念\n",
    "\n",
    "- **`m.weight`** 是特定层（例如 `nn.Linear`, `nn.Conv2d` 等）中的权重参数。在这些层中，权重通常是用 `nn.Parameter` 定义的，因此 `m.weight` 本质上是一个 `nn.Parameter` 对象。\n",
    "\n",
    "- **关联到具体的模型层**：`m.weight` 通常与某个特定的神经网络层相关联，是该层的权重矩阵。它是在构造特定层（如 `nn.Linear`）时被自动创建并初始化的。\n",
    "\n",
    "### 3. 区别与联系\n",
    "\n",
    "- **联系**：`m.weight` 通常是一个 `nn.Parameter`，它是某个具体层（例如 `nn.Linear`）中的权重矩阵。也就是说，`m.weight` 是由 `nn.Parameter` 定义的一个特殊张量。换句话说，`m.weight` 是 `nn.Parameter` 的一个实例。\n",
    "\n",
    "- **区别**：`nn.Parameter` 是一个用于定义任何模型参数的概念工具，而 `m.weight` 是特定层（如 `nn.Linear`）中的权重矩阵。`m.weight` 是 `nn.Parameter` 的具体应用场景。\n",
    "\n",
    "### 4. 示例代码\n",
    "\n",
    "以下代码展示了 `nn.Parameter` 和 `m.weight` 之间的关系：\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyLinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(MyLinearLayer, self).__init__()\n",
    "        # 使用 nn.Parameter 手动定义一个权重参数\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n",
    "        # 偏置参数也可以用 nn.Parameter 手动定义\n",
    "        self.bias = nn.Parameter(torch.randn(out_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x @ self.weight.t() + self.bias\n",
    "\n",
    "# 创建一个自定义的线性层\n",
    "layer = MyLinearLayer(10, 5)\n",
    "\n",
    "# 打印自定义层的权重和偏置\n",
    "print(layer.weight)  # 这是 nn.Parameter 类型的张量\n",
    "print(layer.bias)    # 也是 nn.Parameter 类型的张量\n",
    "\n",
    "# 调用 PyTorch 自带的 nn.Linear 层\n",
    "linear = nn.Linear(10, 5)\n",
    "\n",
    "# 打印 nn.Linear 层的权重和偏置\n",
    "print(linear.weight)  # 这是 nn.Parameter 类型的张量\n",
    "print(linear.bias)    # 也是 nn.Parameter 类型的张量\n",
    "```\n",
    "\n",
    "在这个示例中：\n",
    "\n",
    "- `layer.weight` 是手动使用 `nn.Parameter` 定义的权重张量。\n",
    "- `linear.weight` 是使用 `nn.Linear` 层自动生成的权重张量。虽然方式不同，但它们本质上都是 `nn.Parameter` 对象，并且都参与模型的训练。\n",
    "\n",
    "### 总结\n",
    "\n",
    "- **`nn.Parameter`** 是用于定义可训练模型参数的工具。\n",
    "- **`m.weight`** 是具体层中的权重参数，通常是通过 `nn.Parameter` 创建的张量。\n",
    "- 它们之间的关系是：`m.weight` 是一个由 `nn.Parameter` 定义的特定张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5fc4bfc-3fa0-4f20-a773-8fb6d34a3d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[ 0.1089, -0.2914, -0.4313,  0.1352, -0.1609],\n",
       "         [-0.1652, -0.1341, -0.1260, -0.3020, -0.2400],\n",
       "         [-0.3801,  0.0726, -0.0502,  0.3054,  0.4354],\n",
       "         [ 0.0056,  0.4245,  0.0022, -0.0084, -0.0978]], requires_grad=True),\n",
       " <generator object Module.parameters at 0x000001493B55C740>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "net=nn.Linear(5,4)\n",
    "net.weight,net.parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8284c0a-5e69-4caf-a76c-71affb32e943",
   "metadata": {},
   "source": [
    "`net.weight` 和 `net.parameters()` 都涉及到神经网络模型中的参数管理，但它们的用途和作用范围有所不同。下面解释它们之间的区别：\n",
    "\n",
    "### 1. `net.weight`\n",
    "- **具体的层参数**: `net.weight` 通常指某个特定层（如 `nn.Linear` 或 `nn.Conv2d`）的权重参数。例如，如果 `net` 是一个 `nn.Linear` 层，那么 `net.weight` 就是这个线性层的权重矩阵。\n",
    "\n",
    "- **单个张量**: `net.weight` 是一个 `torch.Tensor` 对象，通常是 `nn.Parameter` 类型，它表示具体层的权重数据。\n",
    "\n",
    "- **示例**:\n",
    "  ```python\n",
    "  import torch.nn as nn\n",
    "\n",
    "  net = nn.Linear(10, 5)  # 创建一个线性层\n",
    "  print(net.weight)  # 打印该层的权重参数\n",
    "  ```\n",
    "\n",
    "  这里的 `net.weight` 直接访问并返回线性层的权重参数。\n",
    "\n",
    "### 2. `net.parameters()`\n",
    "- **整个模型的所有参数**: `net.parameters()` 是一个方法，返回的是模型中所有可训练参数的迭代器。这些参数通常是通过 `nn.Parameter` 定义的，包括权重和偏置（如 `weight` 和 `bias`）。\n",
    "\n",
    "- **用于优化器**: 当你要将模型的所有参数传递给优化器进行训练时，通常会使用 `net.parameters()`。\n",
    "\n",
    "- **示例**:\n",
    "  ```python\n",
    "  import torch.nn as nn\n",
    "\n",
    "  class SimpleModel(nn.Module):\n",
    "      def __init__(self):\n",
    "          super(SimpleModel, self).__init__()\n",
    "          self.fc1 = nn.Linear(10, 5)\n",
    "          self.fc2 = nn.Linear(5, 2)\n",
    "\n",
    "      def forward(self, x):\n",
    "          x = self.fc1(x)\n",
    "          return self.fc2(x)\n",
    "\n",
    "  net = SimpleModel()\n",
    "  for param in net.parameters():\n",
    "      print(param)\n",
    "  ```\n",
    "\n",
    "  在这个示例中，`net.parameters()` 返回的是模型 `net` 中所有层的参数，包括 `fc1` 和 `fc2` 层的权重和偏置。\n",
    "\n",
    "### 3. 区别总结\n",
    "- **范围**:\n",
    "  - `net.weight` 是针对某个特定层的权重参数。\n",
    "  - `net.parameters()` 返回整个模型的所有可训练参数，包括多个层的权重和偏置。\n",
    "\n",
    "- **返回类型**:\n",
    "  - `net.weight` 是一个单独的 `torch.Tensor` 对象。\n",
    "  - `net.parameters()` 是一个生成器对象，生成模型中所有参数的迭代器。\n",
    "\n",
    "- **典型用法**:\n",
    "  - `net.weight` 通常在特定的层上被使用，用于直接访问该层的权重。\n",
    "  - `net.parameters()` 通常用于将整个模型的参数传递给优化器进行训练。\n",
    "\n",
    "### 示例比较\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "net = nn.Linear(10, 5)  # 创建一个线性层\n",
    "\n",
    "# 访问该层的权重\n",
    "print(\"Weight of the layer:\")\n",
    "print(net.weight)\n",
    "\n",
    "# 访问模型的所有参数（这里仅有一个线性层）\n",
    "print(\"\\nAll parameters of the model:\")\n",
    "for param in net.parameters():\n",
    "    print(param)\n",
    "```\n",
    "\n",
    "在上面的例子中：\n",
    "\n",
    "- `net.weight` 只打印该线性层的权重。\n",
    "- `net.parameters()` 则打印了模型中的所有参数，包括权重和偏置。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39701605-1acb-4f73-9c66-a2ad7c1ca228",
   "metadata": {},
   "source": [
    "这段代码展示了如何使用 `TensorDataset` 将数据集分为训练集和测试集。\n",
    "\n",
    "### 代码解释\n",
    "\n",
    "假设你有一个大的数据集，其中 `features` 是特征张量，`labels` 是标签张量。你希望将前 100 个样本作为训练数据，接下来的 100 个样本作为测试数据。通过使用 `TensorDataset`，你可以轻松创建对应的训练和测试数据集。\n",
    "\n",
    "### 详细说明\n",
    "\n",
    "```python\n",
    "train_dataset = TensorDataset(features[0:100], labels[0:100])\n",
    "test_dataset = TensorDataset(features[100:200], labels[100:200])\n",
    "```\n",
    "\n",
    "- **`features[0:100]`**: 取出 `features` 张量中的前 100 个样本，作为训练集的特征。\n",
    "- **`labels[0:100]`**: 取出对应的 `labels` 张量中的前 100 个标签，作为训练集的标签。\n",
    "- **`features[100:200]`**: 取出 `features` 张量中的第 101 到 200 个样本，作为测试集的特征。\n",
    "- **`labels[100:200]`**: 取出对应的 `labels` 张量中的第 101 到 200 个标签，作为测试集的标签。\n",
    "\n",
    "### 结果\n",
    "\n",
    "- **`train_dataset`**: 包含了前 100 个样本及其标签，可以用于模型的训练。\n",
    "- **`test_dataset`**: 包含了接下来的 100 个样本及其标签，可以用于模型的测试。\n",
    "\n",
    "### 后续操作\n",
    "\n",
    "创建了 `TensorDataset` 之后，你可以使用 `DataLoader` 来加载数据进行训练和测试。例如：\n",
    "\n",
    "```python\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# 迭代 train_loader，进行训练\n",
    "for features_batch, labels_batch in train_loader:\n",
    "    # 执行训练操作\n",
    "    pass\n",
    "\n",
    "# 迭代 test_loader，进行测试\n",
    "for features_batch, labels_batch in test_loader:\n",
    "    # 执行测试操作\n",
    "    pass\n",
    "```\n",
    "\n",
    "通过 `DataLoader`，你可以以小批量的形式加载数据，从而在模型训练或测试时节省内存并加快迭代速度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b79d96a-0c5b-44ae-bf56-97417dbfc7b0",
   "metadata": {},
   "source": [
    "`nn.MSELoss()` 是 PyTorch 中用于计算均方误差（Mean Squared Error, MSE）的损失函数。它常用于回归任务中，衡量预测值与真实值之间的平均平方差。\n",
    "\n",
    "### 使用示例\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 创建 MSELoss 对象\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# 假设有两个张量，表示预测值和真实值\n",
    "predictions = torch.tensor([0.5, 0.8, 1.0], dtype=torch.float32)\n",
    "targets = torch.tensor([0.0, 1.0, 1.0], dtype=torch.float32)\n",
    "\n",
    "# 计算损失\n",
    "loss = loss_fn(predictions, targets)\n",
    "\n",
    "print(\"MSE Loss:\", loss.item())\n",
    "```\n",
    "\n",
    "### 解释\n",
    "- `predictions` 是模型输出的预测值。\n",
    "- `targets` 是实际的标签或目标值。\n",
    "- `nn.MSELoss()` 计算预测值与目标值之间的平均平方差异，并返回一个标量值，表示损失。\n",
    "\n",
    "### 计算公式\n",
    "\n",
    "MSE 的公式为：\n",
    "\n",
    "\\[\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "\\]\n",
    "\n",
    "其中 \\( y_i \\) 是目标值， \\( \\hat{y}_i \\) 是预测值， \\( n \\) 是样本的数量。\n",
    "\n",
    "### 应用场景\n",
    "`nn.MSELoss()` 通常用于回归任务中，比如预测连续变量（如房价、温度等）时，计算模型的损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3b5ace-b072-4d17-a2f5-047e452d47f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
